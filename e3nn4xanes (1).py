# -*- coding: utf-8 -*-
"""e3nn4xanes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16RsFT8wC-QsfW3tsBRN5m8g6L4gTF75C

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ninarina12/XANESNN/blob/main/e3nn-xanes.ipynb)

# Predicting K-edge XANES with E(3)NN (Copy)

## Colab Setup

- Go to Runtime > Change runtime type, and select GPU.
- Clone the GitHub repository to access the tutorial files:
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ninarina12/XANESNN.git
# %cd XANESNN

"""- Install the relevant packages:"""

!pip install numpy==1.25
#!pip uninstall torch -y
!pip install torch==2.2
!pip install ase pymatgen cmcrameri e3nn
!pip install torch-scatter torch-cluster torch-sparse torch-spline-conv -f https://pytorch-geometric.com/whl/torch-$(python -c "import torch; print(torch.__version__)").html
!pip install torch-geometric
#installing and importing the materials project infrastructure
#!pip install mp-api

"""## Package imports"""

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import time
import os

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric as tg
import torch_scatter

import keras
from keras import layers
from sklearn.model_selection import train_test_split

from ase import Atom
from ase.data import atomic_masses

from tqdm import tqdm
from utils.data import XANES, Process, bar_format
from utils.e3nn import Network

#from mp_api.client import MPRester
#from emmet.core.xas import Edge, XASDoc, Type

default_dtype = torch.float64
torch.set_default_dtype(default_dtype)
#device = "cuda:0"
device = "cuda:0" if torch.cuda.is_available else "cpu"

if not os.path.exists('images/'):
    os.makedirs('images/')

if not os.path.exists('models/'):
    os.makedirs('models/')

print(np.__version__)

#import json
#from scipy.signal import find_peaks

"""## Load and process data"""

#Also check to see how changing to density changes like everything
#But for now just train the model with the 2300 or so smaples you have
#Good luck

#Grabbing the data from the materials project
with MPRester("od4YR72wa5gNURoZWLf8w1AFriPEIagz", monty_decode=False ,use_document_model=False) as mpr:
    xas_monty = mpr.materials.xas.search(fields=['structure', 'spectrum','formula_pretty'], elements=['V'], absorbing_element='V',edge = 'K')

metals = ["Ti","V","Cr","Mn","Fe","Co","Ni","Cu"]

# Extract only spectrum, structure, and formula
xas = {'structure': [k['spectrum']['structure'] for k in xas_monty],
       'spectrum': [k['spectrum'] for k in xas_monty],
       'formula_pretty': [k['formula_pretty'] for k in xas_monty]
      }


with open('data/V_XANES.json', 'w') as file:
    json.dump(xas, file)

metals = ["Ti","V","Cr","Mn","Fe","Co","Ni","Cu"]
#use only needed fields

metals = ["Ti","V","Cr","Mn","Fe","Co","Ni","Cu"]
xases = metals

for i in tqdm(range(len(metals))):
  with MPRester("od4YR72wa5gNURoZWLf8w1AFriPEIagz", monty_decode=False ,use_document_model=False) as mpr:
    xas_monty = mpr.materials.xas.search(fields=['structure', 'spectrum','formula_pretty'], elements=[metals[i]], absorbing_element=metals[i],edge = 'K')

  xases[i] = {'structure': [k['spectrum']['structure'] for k in xas_monty],
       'spectrum': [k['spectrum'] for k in xas_monty],
       'formula_pretty': [k['formula_pretty'] for k in xas_monty]
      }

counter = 0
metals = ["Ti","V","Cr","Mn","Fe","Co","Ni","Cu"]
for i in range(len(metals)):
  for j in range(len(metals)):
    for k in range(len(xases[i]['formula_pretty'])):
      if j != i:
        if metals[j] in xases[i]['formula_pretty'][k]:
          counter += 1
          xases[i]['formula_pretty'][k] = "remove"
print(counter)

with open("transitional_metals.json", 'w') as file:
    json.dump(xases[0], file)

metals = ["Mn","Fe","Au","Ag","Ti","Pt","Ni","Cr","Hg","Co","W","Cd","Li"]

for i in tqdm(range(len(metals))):
  with MPRester("od4YR72wa5gNURoZWLf8w1AFriPEIagz", monty_decode=False ,use_document_model=False) as mpr:
    xas_monty = mpr.materials.xas.search(elements=[metals[i]], absorbing_element=metals[i],edge = 'K')

  xas = {'structure': [k['spectrum']['structure'] for k in xas_monty],
       'spectrum': [k['spectrum'] for k in xas_monty],
       'formula_pretty': [k['formula_pretty'] for k in xas_monty]
      }
  spectrum_name = metals[i] + "_XANES.json"

  with open(spectrum_name, 'w') as file:
    json.dump(xas, file)

# Load data
#data_file = 'data/Ni_K_XANES_averaged_simplified.json'
element = "Cr"
data_file1 = 'data/'+element + '_XANES'

#full_data_file = 'data/Ni_K_XANES_averaged_full.json'
xanes = XANES()
xanes.load_data(data_file1)
xanes.data

#Analyzing the data before purification
densisx = np.zeros(len(xanes.data))
for i in xanes.data.index:
  densisx[i] = (max(xanes.data['spectrum'][i]['x']) - min(xanes.data['spectrum'][i]['x']))/len(xanes.data['spectrum'][i]['x'])
plt.hist(densisx)
plt.title(element + " XANES Spectra Density Before Purification")
plt.ylabel("Counts")
plt.xlabel("Spectra Density")

#Removing XFAS Data (low density data)
outliers_den = []
for i in xanes.data.index:
  if (max(xanes.data['spectrum'][i]['x']) - min(xanes.data['spectrum'][i]['x']))/len(xanes.data['spectrum'][i]['x']) > 1:
    outliers_den += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
print(len(outliers_den))

#Densities after purification
densisx = np.zeros(len(xanes.data))
for i in xanes.data.index:
  densisx[i] = (max(xanes.data['spectrum'][i]['x']) - min(xanes.data['spectrum'][i]['x']))/len(xanes.data['spectrum'][i]['x'])
plt.hist(densisx,25,(0,1))
plt.title(element + " XANES Spectra Density After Purification")
plt.ylabel("Counts")
plt.xlabel("Spectra Density")

#Removing repeats from the data
repeats = []
length = len(xanes.data)
for i in tqdm(xanes.data.index):
  counter = 0
  for j in range(i+1,length):
    if np.sum(xanes.data['spectrum'][i]['y']) == np.sum(xanes.data['spectrum'][j]['y']):
      if xanes.data['formula_pretty'][i] == xanes.data['formula_pretty'][j]:
        counter += 1
  if counter > 0:
    repeats += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
print("this data has this many repeats",len(repeats))
print("The original length of the data is", len(xanes.data))
original_length = len(xanes.data)

#Absorbance Max before purification
maxisy = np.zeros(len(xanes.data))
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
plt.title(element + " XANES Spectra Max Absorption Before Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Spectra Max")
plt.hist(maxisy,13,(0,13))

#Absorbance Min before purification
minisy = np.zeros(len(xanes.data))
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])
plt.title(element + " XANES Spectra Max Absorption Before Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Spectra Min")
plt.hist(minisy)

#Removing y outliers
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])

y_max_cutoff = np.mean(maxisy) + 5*np.std(maxisy)
y_min_cutoff = np.mean(minisy) - 5*np.std(minisy)
outliers_y = []
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['y']) < y_min_cutoff or max(xanes.data['spectrum'][i]['y']) > y_max_cutoff:
    outliers_y += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
minisy = np.zeros(len(xanes.data))
maxisy = np.zeros(len(xanes.data))
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])
y_max_cutoff = np.mean(maxisy) + 5*np.std(maxisy)
y_min_cutoff = np.mean(minisy) - 5*np.std(minisy)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['y']) < y_min_cutoff or max(xanes.data['spectrum'][i]['y']) > y_max_cutoff:
    outliers_y += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])
y_max_cutoff = np.mean(maxisy) + 5*np.std(maxisy)
y_min_cutoff = np.mean(minisy) - 5*np.std(minisy)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['y']) < y_min_cutoff or max(xanes.data['spectrum'][i]['y']) > y_max_cutoff:
    outliers_y += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])
y_max_cutoff = np.mean(maxisy) + 5*np.std(maxisy)
y_min_cutoff = np.mean(minisy) - 5*np.std(minisy)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['y']) < y_min_cutoff or max(xanes.data['spectrum'][i]['y']) > y_max_cutoff:
    outliers_y += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])
y_max_cutoff = np.mean(maxisy) + 5*np.std(maxisy)
y_min_cutoff = np.mean(minisy) - 5*np.std(minisy)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['y']) < y_min_cutoff or max(xanes.data['spectrum'][i]['y']) > y_max_cutoff:
    outliers_y += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
print("this data has this many y_outliers", len(outliers_y))

print(min(minisy),max(maxisy))

#Absorbance Max after purification
maxisy = np.zeros(len(xanes.data))
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
plt.title(element + " XANES Spectra Max Absorption After Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Spectra Max")
plt.hist(maxisy)

#Absorbance Min after purification
minisy = np.zeros(len(xanes.data))
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])
plt.title(element + " XANES Spectra Max Absorption After Purification")
plt.ylabel("Counts")
plt.xlabel("Spectra Min")
plt.yscale("log")
plt.hist(minisy)

#Analyzing Energy Max before purification
maxisx = np.zeros(len(xanes.data))
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
plt.title(element + " XANES Spectra Max Energy Before Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Energy Max (eV)")
plt.hist(maxisx)

#Analyzing Energy Min before purification
minisx = np.zeros(len(xanes.data))
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
plt.title(element + " XANES Spectra Minimum Energy Before Purification")
plt.ylabel("Log(Counts)")
plt.yscale("log")
plt.xlabel("Energy Minimum (eV)")
plt.hist(minisx)

#Removing  x outliers
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
xmin = np.mean(minisx) + 4*np.std(minisx)
xmax = np.mean(maxisx) - 4*np.std(maxisx)
outliers_x = []
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['x']) > xmin or max(xanes.data['spectrum'][i]['x']) < xmax:
    outliers_x += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
xmin = np.mean(minisx) + 4*np.std(minisx)
xmax = np.mean(maxisx) - 4*np.std(maxisx)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['x']) > xmin or max(xanes.data['spectrum'][i]['x']) < xmax:
    outliers_x += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
xmin = np.mean(minisx) + 4*np.std(minisx)
xmax = np.mean(maxisx) - 4*np.std(maxisx)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['x']) > xmin or max(xanes.data['spectrum'][i]['x']) < xmax:
    outliers_x += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
xmin = np.mean(minisx) + 4*np.std(minisx)
xmax = np.mean(maxisx) - 4*np.std(maxisx)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['x']) > xmin or max(xanes.data['spectrum'][i]['x']) < xmax:
    outliers_x += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
xmin = np.mean(minisx) + 4*np.std(minisx)
xmax = np.mean(maxisx) - 4*np.std(maxisx)
for i in xanes.data.index:
  if min(xanes.data['spectrum'][i]['x']) > xmin or max(xanes.data['spectrum'][i]['x']) < xmax:
    outliers_x += [xanes.data['formula_pretty'][i]]
    xanes.data = xanes.data.drop(index = i)
xanes.data.index = np.arange(0,len(xanes.data))
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
xmin = max(minisx)
xmax = min(maxisx)
print("This data has this many x outliers", len(outliers_x))
print(xmin,xmax)

#Analyzing Energy Max after purification
maxisx = np.zeros(len(xanes.data))
for i in xanes.data.index:
  maxisx[i] = max(xanes.data['spectrum'][i]['x'])
plt.title(element + " XANES Spectra Max Energy After Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Energy Max (eV)")
plt.hist(maxisx)

#Analyzing Energy Min after purification
minisx = np.zeros(len(xanes.data))
for i in xanes.data.index:
  minisx[i] = min(xanes.data['spectrum'][i]['x'])
plt.title(element + " XANES Spectra Max Energy After Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Energy Min (eV)")
plt.hist(minisx)

#Check to see if any radioactive stuff is contained
radio = ('Bi','Po','At','Rn',
              'Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm')
for i in xanes.data.index:
  for j in radio:
    if j in xanes.data['formula_pretty'][i]:
      print(j,i,xanes.data['formula_pretty'][i])

#Removing outlier elements from our dataset
#Only those with less than 10 examples each
#We run through the data multiple time though because sometimes removing data can
#move an element to the outlier range so we must go through it again
elements = ('H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl','Ar','K','Ca','Sc','Ti',
              'V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru',
              'Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb',
              'Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn',
              'Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm')
outliers_e = []
not_included_elements = []
afters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789()'
for t in range(5):
  for e in elements:
    elemental_counter = 0
    for i in xanes.data.index:
      if e in xanes.data['formula_pretty'][i]:
        if len(e) > 1:
          elemental_counter += 1
        else:
            for u in afters:
              if xanes.data['formula_pretty'][i][len(xanes.data['formula_pretty'][i])-1] == e:
                elemental_counter += 1
                break
              if e + u in xanes.data['formula_pretty'][i]:
                elemental_counter += 1
                break
    if elemental_counter < 10:
      if t==4:
        not_included_elements += [e]
      for i in xanes.data.index:
        if e in xanes.data['formula_pretty'][i]:
          if len(e) > 1:
            outliers_e += [xanes.data['formula_pretty'][i]]
            xanes.data = xanes.data.drop(index = i)
          else:
            for u in afters:
              if xanes.data['formula_pretty'][i][len(xanes.data['formula_pretty'][i])-1] == e:
                outliers_e += [xanes.data['formula_pretty'][i]]
                xanes.data = xanes.data.drop(index = i)
                break
              if e + u in xanes.data['formula_pretty'][i]:
                outliers_e += [xanes.data['formula_pretty'][i]]
                xanes.data = xanes.data.drop(index = i)
                break
xanes.data.index = np.arange(0,len(xanes.data))
print("The number of elemental outliers is", len(outliers_e))
print("The list of not included elements is", not_included_elements)
print("the number of missing element is", len(not_included_elements))
print(len(xanes.data))

#Analyzing Max Absorbance After purification
maxisy = np.zeros(len(xanes.data))
for i in xanes.data.index:
  maxisy[i] = max(xanes.data['spectrum'][i]['y'])
plt.title(element + " XANES Spectra Max Absorbance After Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Energy Max (eV)")
plt.hist(maxisy,50)

#Analyzing Min Absorbance After purification
minisy = np.zeros(len(xanes.data))
for i in xanes.data.index:
  minisy[i] = min(xanes.data['spectrum'][i]['y'])
plt.title(element + " XANES Spectra Min Absorbance After Purification")
plt.ylabel("Counts")
plt.yscale("log")
plt.xlabel("Energy Max (eV)")
plt.hist(minisy)

#Getting the data kept percentage
print("we kept this percent of the original data", 100*len(xanes.data)/original_length)

gob = 414
plt.plot(xanes.data['spectrum'][gob]['x'],xanes.data['spectrum'][gob]['y'])
plt.title("XANES Spectra for " + xanes.data['formula_pretty'][gob] + " Before Interpolation")
plt.ylabel("Absorbtion")
plt.xlabel("Energy (eV)")
print(xanes.data['formula_pretty'][gob])

# TO DO: Interpolate XANES data to uniform energy bins
x_new = np.linspace(xmin,xmax,len(xanes.data['spectrum'][0]['y']))
MSE = 0
for i in xanes.data.index:
  x_old = xanes.data['spectrum'][i]['x']
  y_old = xanes.data['spectrum'][i]['y']
  y_new = np.interp(x_new,x_old,y_old)
  xanes.data['spectrum'][i]['x'] = x_new
  xanes.data['spectrum'][i]['y'] = y_new
  y_pred = np.interp(x_old,x_new,y_new)
  MSE += np.mean((y_old-y_pred)**2)
MSE = MSE/len(xanes.data)
#plt.plot(x_old,y_old)
#plt.plot(x_new,y_new)
print(MSE)

plt.plot(xanes.data['spectrum'][gob]['x'],xanes.data['spectrum'][gob]['y'])
plt.title("XANES Spectra for " + xanes.data['formula_pretty'][gob] + " After Interpolation")
plt.ylabel("Absorbtion")
plt.xlabel("Energy (eV)")
print(xanes.data['formula_pretty'][gob])

#Change the y values into peak locations
max_peaks = 0
for i in xa#nes.data.index:
  x_peaks = find_peaks(xanes.data['spectrum'][i]['y'])[0]
  if len(x_peaks) > max_peaks:
    max_peaks = len(x_peaks)
for i in xanes.data.index:
  x_peaks = find_peaks(xanes.data['spectrum'][i]['y'])[0]
  #print(len(x_peaks))
  #y = np.ones(6)*-1
  y_new = np.pad(x_peaks, (0, max_peaks-len(x_peaks)), 'constant',constant_values=(0, -50))/100
  xanes.data['spectrum'][i]['y'] = y_new

gob = 414
geb = 162
plt.plot(xanes.data['spectrum'][gob]['x'],xanes.data['spectrum'][gob]['y'])
plt.title("XANES Spectra for " + xanes.data['formula_pretty'][gob] + " Before Interpolation")
plt.ylabel("Absorbtion")
plt.xlabel("Energy (eV)")
plt.plot(xanes.data['spectrum'][geb]['x'],xanes.data['spectrum'][geb]['y'])
print(xanes.data['formula_pretty'][gob])
print(xanes.data['formula_pretty'][geb])

#Designing the function that can turn formulas into vectors
def element_to_vector(element):
  #Only using the first 100 elements since none past that point are in the sample data
  elements = ('H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl','Ar','K','Ca','Sc','Ti',
              'V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru',
              'Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb',
              'Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn',
              'Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm')
  vector = np.zeros(len(elements))
  counter = 0
  for i in range(len(elements)):
    if element == elements[i]:
      vector[i] = 1
      return vector
    else:
      counter += 1
  if counter ==len(elements):
    raise Exception('Element not found')

def formula_to_vector(formula):
  #This assumes parenthesis only appear once in the entire formula
  upper = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
  lower = 'abcdefghijklmnopqrstuvwxyz'
  length = len(formula)
  vector = np.zeros(100)
  parenthesis = 0
  for i in range(length):
    #Every element starts with an uppercase so we use this method
    if formula[i] != ')':
      if formula[i] != '(':
        if parenthesis == 0:
          if formula[i] in upper:
            if length - 1 > i:
              if formula[i+1] in lower:
                element = formula[i] + formula[i+1]
              else:
                element = formula[i]
            else:
              element = formula[i]
          else:
            continue
        else:
          continue
      else:
        counter = 0
        parenthesis += 1
        vector_mini = formula_to_vector(formula[i+1:])
        for j in range(10,100):
          if ")" + str(j) in formula:
            vector += vector_mini*j
          else:
            counter += 1
        #Then single digits
        if counter == 90:
          for j in range(10):
            if ')' + str(j) in formula:
              vector += vector_mini*j
            else:
              counter += 1
        #Then it just assumes it has no number in front of it
        if counter == 100:
          vector += vector_mini
        continue
    else:
      if parenthesis==0:
        return vector
      else:
        parenthesis -= 1
        continue
    #We know that no element appears in a chemical more than 53 times in our data set
    #But we generalize to 100
    counter = 0
    #This checks for double digits first
    for j in range(10,100):
      if element + str(j) in formula:
        vector += element_to_vector(element)*j
      else:
        counter += 1
    #Then single digits
    if counter == 90:
      for j in range(10):
        if element + str(j) in formula:
          vector += element_to_vector(element)*j
        else:
          counter += 1
    #Then it just assumes it has no number in front of it
    if counter == 100:
      vector += element_to_vector(element)
  return vector

formula_to_vector('He5(H7CV11Es10)2Fm13')

#Making sure there are no double parenthises in the data
counter = 0
for i in range(len(xanes.data)):
  counter = 0
  for j in range(len(xanes.data['formula_pretty'][i])):
   if "(" == xanes.data['formula_pretty'][i][j]:
    counter += 1
  if counter > 1:
    print(counter)
    print(i)
print("This data has this many double parenthesis", counter)

#Calculating minimum distance in our dataset
length = len(xanes.data)
minimum = 1
mins = np.zeros(int(((length-1)**2 + length - 1)/2))
shift = 0
for i in tqdm(range(len(xanes.data)-1)):
  for j in range(i+1,len(xanes.data)):
    mins[shift] = np.linalg.norm(xanes.data['spectrum'][i]['y']-xanes.data['spectrum'][j]['y'])**2/100
    shift += 1

#Getting all the important info
print("The minimum is", min(mins))
print("The 1st percentile is", np.percentile(mins,1))
print("The 2nd percentile is", np.percentile(mins,2))
print("The 5th percentile is", np.percentile(mins,5))
print("The 10th percentile is", np.percentile(mins,10))
print("The 50th percentile is", np.percentile(mins,50))
#print(xanes.data['formula_pretty'][closesti],xanes.data['formula_pretty'][closestj])
print((0,np.percentile(mins,99)))
plt.hist(mins,32)
plt.title("Comparison Lengths Histogram")
plt.ylabel("Counts")
plt.xlabel("MSE")
#plt.plot(xanes.data['spectrum'][closesti]['x'],xanes.data['spectrum'][closesti]['y'])
#plt.plot(xanes.data['spectrum'][closestj]['x'],xanes.data['spectrum'][closestj]['y'])

#Calculating Distances/(average Mean squared) in our dataset
length = len(xanes.data)
minimum = 1
mind = np.zeros(int(((length-1)**2 + length - 1)/2))
shift = 0
for i in tqdm(range(len(xanes.data)-1)):
  for j in range(i+1,len(xanes.data)):
    mind[shift] = np.linalg.norm(xanes.data['spectrum'][i]['y']-xanes.data['spectrum'][j]['y'])**2/100
    mind[shift] = 2*mins[shift]/(np.mean(xanes.data['spectrum'][i]['y'])**2+np.mean(xanes.data['spectrum'][j]['y'])**2)
    shift += 1

#Getting more important info about percentile error
print("The minimum is", min(mind))
print("The 1st percentile is", np.percentile(mind,1))
print("The 2nd percentile is", np.percentile(mind,2))
print("The 5th percentile is", np.percentile(mind,5))
print("The 10th percentile is", np.percentile(mind,10))
print("The 50th percentile is", np.percentile(mind,50))
#print(xanes.data['formula_pretty'][closesti],xanes.data['formula_pretty'][closestj])
print((0,np.percentile(mind,99)))
plt.hist(mind,32)
plt.title("Comparison Lengths Over Mean Histogram")
plt.ylabel("Counts")
plt.xlabel("MSE")
#plt.plot(xanes.data['spectrum'][closesti]['x'],xanes.data['spectrum'][closesti]['y'])
#plt.plot(xanes.data['spectrum'][closestj]['x'],xanes.data['spectrum'][closestj]['y'])

plt.hist(loss,50,(0,np.percentile(loss,99)))
plt.title('Mean Spectrum MSE Histogram for Cu')
plt.ylabel('Count')
plt.xlabel('MSE')

#Insert the code that will seperate the x's and y's for the alternative model
#Creating a constant test set with random train and val sets
test_size = .2
np.random.seed(42)
num_list = list(np.linspace(0,len(xanes.data)-1,len(xanes.data)))
np.random.shuffle(num_list)

chunk = 4
print("Your chunk number is", chunk)

test_index = num_list[0:int(len(xanes.data)*test_size)]
val_train_index = num_list[int(len(xanes.data)*test_size):]
val_index = val_train_index[int(len(val_train_index)*test_size*(chunk-1)):int(len(val_train_index)*test_size*chunk)]
train_index = val_train_index[:int(len(val_train_index)*test_size*(chunk-1))] + val_train_index[int(len(val_train_index)*test_size*chunk):]

#train_index = []
#for i in range(1,int(1/test_size)+1):
  #if i!=chunk:
    #train_index += val_train_index[int(len(val_train_index)*test_size*(i-1)):int(len(val_train_index)*test_size*i)]

print("The training size is", len(train_index))
print("The validation size is", len(val_index))
print("The test size is", len(test_index))

#process.idx_train = train_index
#process.idx_valid = val_index
#process.idx_test = test_index

#defining the elements
elements = ('H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl','Ar','K','Ca','Sc','Ti',
              'V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru',
              'Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb',
              'Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn',
              'Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm')

#Finding the compound with the most elements
max_count = 0
for i in xanes.data.index:
  if len(xanes.data['structure'][i]) > max_count:
    max_count = len(xanes.data['structure'][i])
print(max_count)

#Comparing the average and median MSE as a function of element count
elements = ('H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl','Ar','K','Ca','Sc','Ti',
              'V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru',
              'Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb',
              'Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn',
              'Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm')
afters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789()'
count = np.zeros(100)
elemental = np.arange(100)
elemental_MSE = np.zeros(100)
shift = -1
for e in tqdm(elements):
  shift += 1
  if len(e) == 2:
    for i in train_index:
      if e in xanes.data['formula_pretty'][i]:
        count[shift] += 1
  else:
    for i in train_index:
      for a in afters:
        if e + a in xanes.data['formula_pretty'][i] or e == xanes.data['formula_pretty'][i][len(xanes.data['formula_pretty'][i])-1]:
          count[shift] += 1
          break
print(count)

#Calculating the mean spectrum and the mean and median error from it
means = np.zeros(len(xanes.data['spectrum'][i]['y']))
for i in train_index:
  means += xanes.data['spectrum'][i]['y']
means = means/len(train_index)
plt.plot(x_new,means)
plt.title("Mean Spcectrum for Cu Chunk")
plt.ylabel("Absorbtion")
plt.xlabel("Energy")
loss = np.zeros(len(val_index))
shifter = 0
for i in val_index:
  loss[shifter] += np.mean((xanes.data['spectrum'][i]['y']-means)**2)
  shifter += 1
print("Average MSE of", np.mean(loss))
print("Median MSE of", np.median(loss))
print(len(xanes.data), "examples")

#Creating the inputs for our alternative model
x_train = np.zeros((len(train_index),100))
y_train = np.zeros((len(train_index),100))
shifter = 0
for i in train_index:
    x_train[shifter] = formula_to_vector(xanes.data['formula_pretty'][i])
    y_train[shifter] = xanes.data['spectrum'][i]['y']
    shifter += 1
shifter = 0
x_val = np.zeros((len(val_index),100))
y_val = np.zeros((len(val_index),100))
for i in val_index:
  x_val[shifter] = formula_to_vector(xanes.data['formula_pretty'][i])
  y_val[shifter] = xanes.data['spectrum'][i]['y']
  shifter += 1

print(len(x_val),len(x_train))

#Creating the alternative model
inputs = keras.Input(shape=(100,), name="formula")
x = layers.Dense(100, activation="relu", name="dense_1")(inputs)
x = layers.Dropout(.01)(x)
x = layers.Dense(300, activation="relu", name="dense_2")(x)
x = layers.Dropout(.01)(x)
x = layers.Dense(300, activation="relu", name="dense_3")(x)
outputs = layers.Dense(100, activation="relu", name="Sprectrum")(x)

mymodel = keras.Model(inputs=inputs, outputs=outputs)

#Fitting the model to the data
mymodel.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.MeanSquaredError(),
    metrics=['accuracy'],
)
history = mymodel.fit(
    x_train,
    y_train,
    batch_size=64,
    epochs=150,
    validation_data=(x_val, y_val),
)

#Looking at the training history
# list all data in history
print(history.history.keys())
plt.plot(history.history['loss'],)
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.yscale('log')
plt.legend(['train', 'val'], loc='upper left')
plt.ylim((0.01,.1))
plt.show()

#Getting the important alternative model data
print(min(history.history['val_loss']))

mymodel.save(element+'_formulaic_model.keras')

#Loading a specific model
#mymodel = keras.models.load_model('data/Cu_formulaic_model.keras')

mymodel.predict(np.ones((1,100)))[0] - 10

#Getting the MSE of the test set
test_MSE = np.zeros(len(test_index))
shifter = 0
for i in test_index:
  test_input = np.ones((1,100))
  test_input[0] = formula_to_vector(xanes.data['formula_pretty'][i])
  test_MSE[shifter] = np.mean((mymodel.predict(test_input)[0] - xanes.data['spectrum'][i]['y'])**2)
  shifter += 1

print(element)
print(np.mean(test_MSE),np.median(test_MSE))

test_MAE = np.zeros(len(test_index))
shifter = 0
for i in test_index:
  test_input = np.ones((1,100))
  test_input[0] = formula_to_vector(xanes.data['formula_pretty'][i])
  test_MAE[shifter] = np.mean(abs(mymodel.predict(test_input)[0] - xanes.data['spectrum'][i]['y']))
  shifter += 1

print(element)
print(np.mean(test_MAE),np.median(test_MAE))

#Getting the elemental count of the training set
elements = ('H','He','Li','Be','B','C','N','O','F','Ne','Na','Mg','Al','Si','P','S','Cl','Ar','K','Ca','Sc','Ti',
              'V','Cr','Mn','Fe','Co','Ni','Cu','Zn','Ga','Ge','As','Se','Br','Kr','Rb','Sr','Y','Zr','Nb','Mo','Tc','Ru',
              'Rh','Pd','Ag','Cd','In','Sn','Sb','Te','I','Xe','Cs','Ba','La','Ce','Pr','Nd','Pm','Sm','Eu','Gd','Tb',
              'Dy','Ho','Er','Tm','Yb','Lu','Hf','Ta','W','Re','Os','Ir','Pt','Au','Hg','Tl','Pb','Bi','Po','At','Rn',
              'Fr','Ra','Ac','Th','Pa','U','Np','Pu','Am','Cm','Bk','Cf','Es','Fm')
afters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789()'
count = np.zeros(100)
shift = -1
for e in tqdm(elements):
  shift += 1
  if len(e) == 2:
    for i in train_index:
      if e in xanes.data['formula_pretty'][i]:
        count[shift] += 1
  else:
    for i in train_index:
      for a in afters:
        if e + a in xanes.data['formula_pretty'][i] or e == xanes.data['formula_pretty'][i][len(xanes.data['formula_pretty'][i])-1]:
          count[shift] += 1
          break
print(count)

#Getting the average MSE for each element
elemental_MSE = np.zeros(100)
shift = -1
inputy = np.ones((1,100))
for e in tqdm(elements):
  shift += 1
  if len(e) == 2:
    for u, i in enumerate(test_index):
      if e in xanes.data['formula_pretty'][i]:
        inputy[0] = formula_to_vector(xanes.data['formula_pretty'][i])
        elemental_MSE[shift] += test_MSE[u]/count[shift]
  else:
    for u, i in enumerate(test_index):
      for a in afters:
        if e + a in xanes.data['formula_pretty'][i] or e == xanes.data['formula_pretty'][i][len(xanes.data['formula_pretty'][i])-1]:
          inputy[0] = formula_to_vector(xanes.data['formula_pretty'][i])
          elemental_MSE[shift] += test_MSE[u]/count[shift]
          break
print(elemental_MSE)

#zeroing the element we are on and plotting the correlation
for i,e in enumerate(elements):
  if e == element:
    elemental_MSE[i] = 0
    count[i] = 0
    print(e)
shift = 0
for i in range(100):
  if count[shift] == 0:
    count = np.delete(count,shift)
    elemental_MSE = np.delete(elemental_MSE,shift)
  else:
    shift += 1

coeffs = np.polyfit(np.log(count),elemental_MSE,1)
plt.plot(np.log(count),coeffs[1] + coeffs[0]*np.log(count), label="line of best fit")
plt.plot(np.log(count),elemental_MSE, 'o', label="test set examples")
plt.title("Avergae MSE as a function of the log of each Count")
plt.ylabel("Average MSE (Absorbance Sqaured)")
plt.legend()
plt.xlabel("log(Training Examples)")

#Getting the lin of best fit
np.polyfit(np.log(count),elemental_MSE,1)

#Calculating the mean spectrum and the mean and median error from it
means = np.zeros(len(xanes.data['spectrum'][i]['y']))
for i in train_index:
  means += xanes.data['spectrum'][i]['y']
means = means/len(train_index)
plt.plot(x_new,means)
plt.title("Mean Spcectrum for Cu Training Set")
plt.ylabel("Absorbtion")
plt.xlabel("Energy (eV)")
loss = np.zeros(len(test_index))
print(xanes.data['spectrum'][409]['y'])
shifter = 0
for i in test_index:
  loss[shifter] += np.mean((xanes.data['spectrum'][i]['y']-means)**2)
  shifter += 1
print("Average MSE of", np.mean(loss))
print("Median MSE of", np.median(loss))
print(len(xanes.data), "examples")

plt.hist(test_MSE,25,(0,np.percentile(test_MSE,95)))
print(np.mean(test_MSE),np.median(test_MSE))
plt.title("MSE for the Cu Test Set")
plt.ylabel("Counts")
plt.xlabel("MSE")

plt.hist(test_MAE,25,(0,np.percentile(test_MAE,95)))
print(np.mean(test_MAE),np.median(test_MAE))
plt.title("MAE for the Cu Test Set")
plt.ylabel("Counts")
plt.xlabel("MAE")

for i in range(20):
  placer = int(test_index[i])
  test_input[0] = formula_to_vector(xanes.data['formula_pretty'][placer])
  print(np.mean((mymodel.predict(test_input)[0] - xanes.data['spectrum'][placer]['y'])**2))
plt.plot((mymodel.predict(test_input)[0]))
plt.plot(xanes.data['spectrum'][placer]['y'])

placer = int(test_index[5])
test_input[0] = formula_to_vector(xanes.data['formula_pretty'][placer])
print(np.mean((mymodel.predict(test_input)[0] - xanes.data['spectrum'][placer]['y'])**2))
plt.plot(x_new,(mymodel.predict(test_input)[0]))
plt.plot(x_new,xanes.data['spectrum'][placer]['y'])
plt.title("Example of Median MSE for Cu")
plt.ylabel("Absorbance")
plt.xlabel("Energy (eV)")

#Calculating minimum distance in our dataset
length = len(test_index)
mints = np.zeros(int(((length-1)**2 + length - 1)/2))
shift1 = 0
shift2 = 0
for i in tqdm(test_index):
  for j in test_index[shift1+1:]:
    mints[shift2] = np.mean((xanes.data['spectrum'][int(i)]['y']-xanes.data['spectrum'][int(j)]['y'])**2)
    shift2 += 1
  shift1 += 1
print(shift)

#Getting more important info
print("The minimum is", min(mints))
print("The 1st percentile is", np.percentile(mints,1))
print("The 2nd percentile is", np.percentile(mints,2))
print("The 5th percentile is", np.percentile(mints,5))
print("The 10th percentile is", np.percentile(mints,10))
print("The 50th percentile is", np.percentile(mints,50))
#print(xanes.data['formula_pretty'][closesti],xanes.data['formula_pretty'][closestj])
print((0,np.percentile(mints,90)))
plt.hist(mints,32,(0,np.percentile(mints,95)))
plt.title("MSD for the Cu Test Set")
plt.ylabel("Counts")
plt.xlabel("MSD")
#plt.plot(xanes.data['spe
print(max(mints))



#Evaluating the alternative model
alt_test_error = np.zeros((1,len(test_index)))
inputer = np.zeros((1,100))
for i in tqdm(test_index):
  inputer[0] = formula_to_vector(xanes.data['formula_pretty'][i])
  y_temp = mymodel.predict(inputer)[0]
  mean_dif += np.mean(y_temp-xanes.data['spectrum'][i]['y'])**2

mean_dif = mean_dif/len(test_index)

# Enforce a minimum number of examples of each species
species_min = 0

xanes.get_species_counts()
fig = xanes.plot_species_counts(species_min)
#xanes.set_species_counts(species_min)
#xanes.get_species_counts()
#xanes.savefig('images/species_counts.svg', bbox_inches='tight')

# Lattice parameter statistics
xanes.get_lattice_parameters()
fig = xanes.plot_lattice_parameters(n_bins=50)
#fig.savefig('images/lattice_parameters.svg', bbox_inches='tight')

"""## Format input features"""

# Get species
species = sorted(list(set(xanes.data['species'].sum())))
n_species = list(np.unique(xanes.data['species'].sum(), return_counts=True)[1])
Z_max = max([Atom(k).number for k in species])
print(Z_max)

# One-hot encoding atom type and mass
type_encoding = {}
mass_specie = []

for Z in tqdm(range(1, Z_max + 1), bar_format=bar_format):
    specie = Atom(Z)
    type_encoding[specie.symbol] = Z - 1
    mass_specie.append(atomic_masses[Z])

type_onehot = torch.eye(len(type_encoding))
mass_onehot = torch.diag(torch.tensor(mass_specie))

# Process data into input descriptors
process = Process(species, Z_max, type_encoding, type_onehot, mass_onehot, default_dtype)

r_max = 3.5     # cutoff radius
tqdm.pandas(desc='Building data', bar_format=bar_format)
xanes.data['input'] = xanes.data.progress_apply(lambda x: process.build_data(x, r_max), axis=1)

"""## Format training, validation, and test sets"""



# Train/valid/test split
#test_size = 0.2
#fig = process.train_valid_test_split(xanes.data, valid_size=test_size, test_size=test_size, plot=True)
#fig.savefig('images/train_valid_test_split.svg', bbox_inches='tight')

#The better organizer of data
process.idx_train = train_index
process.idx_valid = val_index
process.idx_test = test_index

# Calculate average number of neighbors
process.get_neighbors(xanes.data)
fig = process.plot_neighbors(n_bins=50)
print('Average number of neighbors (train/valid/test):', process.n_train.mean(), '/',
                                                         process.n_valid.mean(), '/',
                                                         process.n_test.mean())
#fig.savefig('images/num_neighbors.svg', bbox_inches='tight')

# Format dataloaders
#You can replace this
batch_size = 32
dataloader_train = tg.loader.DataLoader(xanes.data.iloc[process.idx_train]['input'].tolist(), batch_size=batch_size,
                                        shuffle=True)
dataloader_valid = tg.loader.DataLoader(xanes.data.iloc[process.idx_valid]['input'].tolist(), batch_size=batch_size)
dataloader_test = tg.loader.DataLoader(xanes.data.iloc[process.idx_test]['input'].tolist(), batch_size=batch_size)

"""## Build neural network model"""

class E3NN(Network):
    def __init__(self, in_dim, out_dim, emb_dim, num_layers, mul, lmax, max_radius, num_basis, radial_layers,
                 radial_neurons, num_neighbors):
        kwargs = {'reduce_output': False,
                  'irreps_in': str(emb_dim)+"x0e",
                  'irreps_out': str(out_dim)+"x0e",
                  'irreps_node_attr': str(emb_dim)+"x0e",
                  'layers': num_layers,
                  'mul': mul,
                  'lmax': lmax,
                  'max_radius': max_radius,
                  'number_of_basis': num_basis,
                  'radial_layers': radial_layers,
                  'radial_neurons': radial_neurons,
                  'num_neighbors': num_neighbors
                 }
        super().__init__(**kwargs)

        # definitions
        self.cmap = plt.get_cmap('plasma')
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.emb_dim = emb_dim
        self.num_layers = num_layers
        self.mul = mul
        self.lmax = lmax
        self.max_radius = max_radius
        self.num_basis = num_basis
        self.radial_layers = radial_layers
        self.radial_neurons = radial_neurons
        self.num_neighbors = num_neighbors

        self.model_name = 'e3nn-xanes_' + '_'.join(i + str(int(j)) for (i,j) in zip(
            ['emb', 'layers', 'mul', 'lmax', 'rmax', 'nbasis', 'rlayers', 'rneurons'],
            [emb_dim, num_layers, mul, lmax, max_radius, num_basis, radial_layers, radial_neurons]))

        # embedding
        self.emb_x = nn.Sequential(
            nn.Linear(in_dim, emb_dim),
            nn.ReLU()
        )

        self.emb_z = nn.Sequential(
            nn.Linear(in_dim, emb_dim),
            nn.Tanh()
        )


    def forward(self, data):
        data['x'] = self.emb_x(data['x_in'])
        data['z'] = self.emb_z(data['z_in'])
        x = super().forward(data)[0]

        # aggregate
        if 'batch' in data:
            batch = data['batch']
        else:
            batch = data['pos'].new_zeros(data['pos'].shape[0], dtype=torch.long)

        y = torch_scatter.scatter_mean(x, batch, dim=0)
        return y


    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


    def loss(self, y_pred, y_true):
        return torch.mean((y_pred-y_true)**2)


    def checkpoint(self, dataloader, device):
        self.eval()

        loss_cum = 0.
        with torch.no_grad():
            for j, d in enumerate(dataloader):
                d.to(device)
                y_pred = self.forward(d)

                loss = self.loss(y_pred, d.y).cpu()
                loss_cum += loss.detach().item()

        return loss_cum/len(dataloader)


    def fit(self, opt, dataloader_train, dataloader_valid, history, s0, max_iter=10, device="cpu", scheduler=None):
        chkpt = 1

        for step in range(max_iter):
            self.train()

            loss_cum = 0.
            start_time = time.time()

            for j, d in enumerate(dataloader_train):
                d.to(device)
                y_pred = self.forward(d)

                loss = self.loss(y_pred, d.y).cpu()
                loss_cum += loss.detach().item()

                print(f"Iteration {step+1:5d}    batch {j+1:5d} / {len(dataloader_train):5d}   " +
                      f"batch loss = {loss.data:.4e}", end="\r", flush=True)

                opt.zero_grad()
                loss.backward()
                opt.step()

            if scheduler is not None:
                scheduler.step()

            end_time = time.time()
            wall = end_time - start_time

            if (step+1)%chkpt == 0:
                print(f"Iteration {step+1:5d}    batch {j+1:5d} / {len(dataloader_train):5d}   " +
                      f"epoch loss = {loss_cum/len(dataloader_train):.4e}")

                loss_valid = self.checkpoint(dataloader_valid, device)
                loss_train = self.checkpoint(dataloader_train, device)

                history.append({
                    'step': step + s0,
                    'wall': wall,
                    'batch': {
                        'loss': loss.item(),
                    },
                    'valid': {
                        'loss': loss_valid,
                    },
                     'train': {
                         'loss': loss_train,
                     },
                })

                yield {
                    'history': history,
                    'state': self.state_dict(),
                    'optimizer': opt.state_dict(),
                    'scheduler': scheduler.state_dict() if scheduler else None
                }

args_enn = {'in_dim': Z_max,
            'out_dim': xanes.data.iloc[0]['input'].y.shape[-1],
            'emb_dim': 64,
            'num_layers': 2,
            'mul': 32,
            'lmax': 2,
            'max_radius': r_max,
            'num_basis': 10,
            'radial_layers': 1,
            'radial_neurons': 100,
            'num_neighbors': process.n_train.mean(),
          }

enn = E3NN(**args_enn).to(device)
opt = torch.optim.Adam(enn.parameters(), lr=1e-3)
scheduler = None #torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.99)

model_num = 0
model_path = 'models/' + enn.model_name + '_' + str(model_num) + '.torch'

print(model_path)
#print(enn)
print('Number of parameters:', enn.count_parameters())

fig = enn.visualize()

"""## Train model"""

resume = False

if resume:
    saved = torch.load(model_path, map_location=device)
    enn.load_state_dict(saved['state'])
    opt.load_state_dict(saved['optimizer'])
    try:
        scheduler.load_state_dict(saved['scheduler'])
    except:
        scheduler = None
    history = saved['history']
    s0 = history[-1]['step'] + 1

else:
    history = []
    s0 = 0

# fit E3NN
  for results in enn.fit(opt, dataloader_train, dataloader_valid, history, s0, max_iter=50, device=device,
                        scheduler=scheduler):
      with open(model_path, 'wb') as f:
          torch.save(results, f)

if not os.path.exists('images/' + enn.model_name + '_' + str(model_num)):
    os.makedirs('images/' + enn.model_name + '_' + str(model_num))

saved = torch.load(model_path, map_location=device)
history = saved['history']

steps = [d['step'] + 1 for d in history]
loss_train = [d['train']['loss'] for d in history]
loss_valid = [d['valid']['loss'] for d in history]

fig, ax = plt.subplots(figsize=(3.5,3))
ax.plot(steps, loss_train, label='Train', color=process.colors['Train'])
ax.plot(steps, loss_valid, label='Valid.', color=process.colors['Valid.'])

ax.set_xlabel('Iterations')
ax.set_ylabel('Loss')
#ax.set_ylim([0,1])
ax.set_title(element+' XANES')
ax.legend(frameon=False)
ax.set_yscale('log')
fig.savefig('images/' + enn.model_name + '_' + str(model_num) + '/loss.svg', bbox_inches='tight')

saved = torch.load(model_path, map_location=device)
history = saved['history']

steps = [d['step'] + 1 for d in history]
loss_train = [d['train']['loss'] for d in history]
loss_valid = [d['valid']['loss'] for d in history]

fig, ax = plt.subplots(figsize=(3.5,3))
ax.plot(steps[10:], loss_train[10:], label='Train', color=process.colors['Train'])
ax.plot(steps[10:], loss_valid[10:], label='Valid.', color=process.colors['Valid.'])

ax.set_xlabel('Iterations')
ax.set_ylabel('Loss')
#ax.set_ylim([0,1])
ax.set_title(element + ' XANES')
ax.legend(frameon=False)
ax.set_yscale('log')
fig.savefig('images/' + enn.model_name + '_' + str(model_num) + '/loss.svg', bbox_inches='tight')



"""## Evaluate model"""

#Comparing some
test_y_true = np.zeros((batch_size*len(dataloader_test),100))
test_y_pred = np.zeros((batch_size*len(dataloader_test),100))
current = 0

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

with torch.no_grad():
  for j,d in enumerate(dataloader_test):
    inputs = d.to(device)  # Adjust 'input' according to your data keys
    targets = d['y'].to(device)

    test_y_pred[current:current+len(d.y)] = enn(d).cpu().numpy()
    test_y_true[current:current+len(d.y)] = targets.cpu().numpy()
    current += len(d.y)

#Getting the cosine similarity
print('The Cosine similarity is')
print(np.sum(test_y_pred*test_y_true)/((np.sum(test_y_pred**2))**(1/2)*(np.sum(test_y_true**2))**(1/2)))

#Find the average error:
error = np.zeros(len(test_y_pred))
for i in range(len(test_y_pred)):
  error[i] = np.mean((test_y_pred[i]-test_y_true[i])**2)
  #if np.mean((test_y_pred[i]-test_y_true[i])**2) == 7.898989:
    #plt.plot(x_new,test_y_pred[i])
    #plt.plot(x_new,test_y_true[i])
print("Quartile 1 is", np.percentile(error,25))
print("The median is", np.median(error))
print("Quartile 3 is", np.percentile(error,75))
print(("The mean is"), np.mean(error))

#Find the average error:
error = np.zeros(len(test_y_pred))
for i in range(len(test_y_pred)):
  error[i] = np.mean(abs(test_y_pred[i]-test_y_true[i]))
  #if np.mean((test_y_pred[i]-test_y_true[i])**2) == 7.898989:
    #plt.plot(x_new,test_y_pred[i])
    #plt.plot(x_new,test_y_true[i])
print("Quartile 1 is", np.percentile(error,25))
print("The median is", np.median(error))
print("Quartile 3 is", np.percentile(error,75))
print(("The mean is"), np.mean(error))

#Calculating the mean spectrum and the mean and median error from it (From the test and val set)
means = np.zeros(len(xanes.data['spectrum'][i]['y']))
for i in train_index+val_index:
  means += xanes.data['spectrum'][i]['y']
means = means/len(train_index)
plt.plot(x_new,means)
plt.title("Mean Spcectrum for "+element)
plt.ylabel("Absorbtion")
plt.xlabel("Energy")
loss = np.zeros(len(test_index))
shifter = 0
for i in test_index:
  loss[shifter] += np.mean((xanes.data['spectrum'][i]['y']-means)**2)
  shifter += 1
print("Average MSE of", np.mean(loss))
print("Median MSE of", np.median(loss))
print(len(xanes.data), "examples")

#Calculating the mean spectrum and the mean and median error from it (Fir the test set)
means = np.zeros(len(xanes.data['spectrum'][i]['y']))
for i in train_index+val_index:
  means += xanes.data['spectrum'][i]['y']
means = means/len(train_index)
plt.plot(x_new,means)
plt.title("Mean Spcectrum for "+element)
plt.ylabel("Absorbtion")
plt.xlabel("Energy")
loss = np.zeros(len(test_index))
shifter = 0
for i in test_index:
  loss[shifter] += np.mean(abs(xanes.data['spectrum'][i]['y']-means))
  shifter += 1
print("Average MAE of", np.mean(loss))
print("Median MAE of", np.median(loss))
print(len(xanes.data), "examples")

plt.plot(x_new,test_y_pred[100])
plt.plot(x_new,test_y_true[100])
plt.title("E(3)NN Prediction vs True Value for " + xanes.data['formula_pretty'][int(test_index[100])])
plt.ylabel('Intensity (A.U.)')
plt.xlabel('Energy (eV)')
print("MSE =",np.mean((test_y_pred[100]-test_y_true[100])**2) )
#print(max(error))
#print(min(error[:len(error)-10]))

plt.hist(error,25,(0,np.percentile(error,99)))
plt.title('Model Error Histogram for '+element)
plt.ylabel('Count')
plt.xlabel('Error')
print("Average MSE of", np.mean(error))
print("Median MSE of", np.median(error))

#Smoothing out the predictions through convolution
#for i in range(len(test_y_pred)):
  temp = np.append(test_y_pred[i][0],test_y_pred[i])
  temp = np.append(temp,test_y_pred[i][len(test_y_pred[i])-1])
  temp = np.convolve(temp,[.25,.5,.25])
  temp = np.delete(temp,len(temp)-1)
  temp = np.delete(temp,len(temp)-1)
  temp = np.delete(temp,0)
  test_y_pred[i] = np.delete(temp,0)